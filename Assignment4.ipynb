{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PetersServers/Assignments/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "id": "HhmY7I5M8VJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c948a15-e91b-45be-bf38-a3cc135a39ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-4cCMruHXaLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Artificial Neural Networks\n",
        "\n",
        "Please read the introdcution of neuronal networks of the book *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*, p. 299-316.\n",
        "\n",
        "Why have neural networks, even though they were invented early on, only now caught on?\n",
        "\n",
        "What is a percepton and a threshold logic unit (TLU)? Try to define a linear function and a step function of your choice, use some values of your choice and explain what might be the result of the percepton. (maybe using max. two TLU's)\n",
        "\n",
        "What is a fully connected layer and a output layer? Why can we easily combine the equations of multiple instances into a fully connected layer?\n",
        "\n",
        "What problem did Marvin Minsky and Seymour Paper highlight that perceptrons could not solve? What is a possible solution?\n",
        "\n",
        "What is a deep neuronal network? What are hidden layers? What means feedforward neural network (FNN).\n",
        "\n",
        "Try to explain how backpropagation works! (In Addition, you can have a look to the following example, which tries manually to compute the backprogation of a simple linear network. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ OR you can also read through the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy))\n",
        "\n",
        "Why do we need activation functions, wouldn't it be easier just using linear functions?\n",
        "\n",
        "## Ideas for the learning portfolio: \n",
        "\n",
        "1) For example, you could train a single TLU to classify iris flowers based on petal length and width in the !!!pyTorch!! environment.\n",
        "\n",
        "2) You could add to our king county housepricing ML project a neuronal network and compare it to the other models. "
      ],
      "metadata": {
        "id": "_Rdj49uwjuoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tThe recent rise in the popularity of neural networks is due to the increasing availability of large amounts of data, faster computing power, and improvements in algorithm development. These advancements have made neural networks more efficient and effective, which has led to their wider use in various applications.\n",
        "2.\tA perceptron is a type of neural network that consists of a single layer of threshold logic units (TLUs). A TLU is a neuron that computes a weighted sum of its inputs, adds a bias term, and applies a step function to the result. A step function is a binary function that outputs 1 or 0 based on whether its input is above or below a certain threshold. For example, a step function with a threshold of 0 would output 1 for any input greater than 0 and 0 for any input less than or equal to 0. We can use two TLUs with different weights and biases to implement a logical operation like AND, OR, or XOR.\n",
        "3.\tA fully connected layer is a layer in a neural network where each neuron is connected to every neuron in the previous layer. An output layer is the final layer of a neural network that produces the network's output (in keras I wouls call it the “dense layer”). We can easily combine the equations of multiple instances into a fully connected layer because each neuron in the layer uses the same input vector but with different weights and biases.\n",
        "4.\tMarvin Minsky and Seymour Papert pointed out that perceptrons could not solve non-linearly separable problems. One potential solution was to use multi-layer neural networks with non-linear activation functions, which could handle non-linearly separable problems. \n",
        "5.\tA deep neural network is a neural network that has multiple hidden layers. Hidden layers perform intermediate computations between input and output layers. A feedforward neural network is a type of neural network where information flows from the input layer to the output layer without any feedback connections.\n",
        "6.\tBackpropagation is an algorithm that trains neural networks by computing the gradient of a loss function with respect to the network's weights and then using this gradient to update the weights to minimize the loss. The algorithm uses the chain rule of calculus to compute the gradient of the loss function with respect to each weight in the network.\n",
        "7.\tActivation functions are crucial in neural networks because they add non-linearity to the model. Without non-linearity, a neural network would be a linear combination of its inputs, which is not sufficient for modeling complex patterns in data. Activation functions allow neural networks to learn non-linear relationships between inputs and outputs, making them more effective for tasks like image recognition, natural language processing, and speech recognition.\n",
        "\n"
      ],
      "metadata": {
        "id": "6b5j5dUeu2C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7AWIRmT3XW2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "4tF8YDV3T91n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A traditional approach: training a digit classifier and learning pyTorch tensors.\n",
        "\n",
        "For this assignment, I ask you to read the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy) to the beginning of the chapter *Stochastic Gradient Descent (SGD)*. \n",
        "\n",
        "First, try to summarize what we know about pyTorch tensors by trying to predict whether we have a 1 or a 7 in the MNIST dataset using a traditional rule-based programming approach. Therefore use pyTorch tensors for the entire tasks and fulfill the following steps:\n",
        "\n",
        "1) Randomly split the MNIST dataset (1 and 7) into a training dataset and a test dataset in a ratio of 80:20.\n",
        "\n",
        "2) Instead of using an optimal 1 or 7 with the mean over the training dataset, try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm. \n",
        "\n",
        "3) For each instance in the test set, decide if it is a 1 or 7 and calculate the precision.\n",
        "\n",
        "Do we get a similar good result?\n"
      ],
      "metadata": {
        "id": "h6OwXNEeed93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Tensors are arrays or matrices with multiple dimensions, like Numpy arrays, which are designed to work efficiently with deep learning algorithms. They are the primary building block in PyTorch, representing input data, intermediate computations, and model parameters. Tensors can be created on the CPU or GPU and manipulated using various mathematical operations such as matrix multiplication, addition, and subtraction. PyTorch also provides a range of functions for manipulating tensors, like slicing, indexing, and concatenation. Tensors support automatic differentiation, allowing gradients to be computed automatically during backpropagation, making them useful for deep learning models. Additionally, PyTorch tensors are highly compatible with NumPy, simplifying the conversion between the two libraries. Overall, PyTorch tensors are a versatile and powerful data structure that form the foundation of many deep learning applications."
      ],
      "metadata": {
        "id": "0i3ZAh_ovKJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Hrrgv9OVebAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "db55310a-ebc9-46e8-c083-357512bd6666"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='15687680' class='' max='15683414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.03% [15687680/15683414 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "path = untar_data(URLs.MNIST)\n",
        "Path.BASE_PATH = path\n",
        "(path/'training').ls()\n",
        "ones = (path/'training'/'1').ls().sorted()\n",
        "sevens = (path/'training'/'7').ls().sorted()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_image(path):\n",
        "  img = Image.open(path)\n",
        "  return np.array(img)\n",
        "\n",
        "X_ones = np.array([load_image(p)for p in ones])\n",
        "\n",
        "X_sevens = np.array([load_image(p) for p in sevens])\n",
        "\n",
        "y_ones = np.full(len(X_ones), 1)\n",
        "y_sevens = np.full(len(X_sevens), 7)\n",
        "\n",
        "X = np.concatenate([X_ones, X_sevens])\n",
        "y = np.concatenate([y_ones, y_sevens])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "print(f\"Train set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXGou4PAZYHU",
        "outputId": "7cce399d-212b-4f84-8b42-5d50d06ad703"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: (2601, 28, 28)\n",
            "Test set size: (10406, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "\n",
        "print(f\"Train set type: {type(X_train)}\")\n",
        "print(f\"Test set type: {type(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDkqZzk2gR1J",
        "outputId": "5a97ff9f-dfa1-48d4-9558-0912fd618ade"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set type: <class 'torch.Tensor'>\n",
            "Test set type: <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test = X_train.float(), X_test.float()\n",
        "y_train, y_test = y_train.float(), y_test.float()\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "y_train /= 255\n",
        "y_test /= 255\n",
        "\n",
        "for name, data in [(\"Train set\", X_train), (\"Test set\", X_test), (\"Train labels\", y_train), (\"Test labels\", y_test)]:\n",
        "    print(f\"{name} min: {data.min()}\")\n",
        "    print(f\"{name} max: {data.max()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocHF17BNggoS",
        "outputId": "5f16ed40-954e-43b5-88f0-36ed75d2e3eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set min: 0.0\n",
            "Train set max: 1.0\n",
            "Test set min: 0.0\n",
            "Test set max: 1.0\n",
            "Train labels min: 0.003921568859368563\n",
            "Train labels max: 0.027450980618596077\n",
            "Test labels min: 0.003921568859368563\n",
            "Test labels max: 0.027450980618596077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mnist_distance(a, b):\n",
        "    return ((a - b) ** 2).sum((-1, -2))\n",
        "\n",
        "distances = []\n",
        "for x in X_test:\n",
        "    distances.append([mnist_distance(x, xt) for xt in X_train])\n",
        "\n",
        "\n",
        "y_pred = []\n",
        "for dist in distances:\n",
        "    sum1 = sum([d for i, d in enumerate(dist) if y_train[i] == 1])\n",
        "    sum7 = sum([d for i, d in enumerate(dist) if y_train[i] == 7])\n",
        "    if sum1 < sum7:\n",
        "        y_pred.append(1)a\n",
        "    else:\n",
        "        y_pred.append(7)\n",
        "\n",
        "precision = (y_pred == y_test).sum().item() / len(y_test)\n",
        "print(\"Precision:\", precision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "zMyblr3zh8DZ",
        "outputId": "eaa2c047-e7df-47fe-dd58-2ef1388edf20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-165568682c40>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Move tensors to GPU device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "def mnist_distance(a, b):\n",
        "    return ((a - b) ** 2).sum((-1, -2))\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach().to('cuda')\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach().to('cuda')\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long).clone().detach().to('cuda')\n",
        "\n",
        "distances = mnist_distance(X_test_tensor[:, None], X_train_tensor)\n",
        "sum1 = (distances[:, y_train_tensor == 1]).sum(axis=1)\n",
        "sum7 = (distances[:, y_train_tensor == 7]).sum(axis=1)\n",
        "y_pred = torch.where(sum1 < sum7, torch.tensor(1, dtype=torch.long).to('cuda'), torch.tensor(7, dtype=torch.long).to('cuda'))\n",
        "\n",
        "precision = (y_pred == torch.tensor(y_test, dtype=torch.long).to('cuda')).sum().item() / len(y_test)\n",
        "print(\"Precision:\", precision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "GdN0bQxykZ8-",
        "outputId": "519015bc-62e7-4e54-a73a-4a9e27666c05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-a04120765655>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).clone().detach().to('cuda')\n",
            "<ipython-input-11-a04120765655>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32).clone().detach().to('cuda')\n",
            "<ipython-input-11-a04120765655>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train_tensor = torch.tensor(y_train, dtype=torch.long).clone().detach().to('cuda')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a04120765655>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msum1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msum7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a04120765655>\u001b[0m in \u001b[0;36mmnist_distance\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmnist_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_test_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 79.05 GiB (GPU 0; 14.75 GiB total capacity; 119.38 MiB already allocated; 14.51 GiB free; 138.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent (SGD)\n",
        "\n",
        "For this exercise I ask you to read the chapter Stochastic Gradient Descent (SGD) from the Google Colab 04_mnist_basics.ipynb in paralell. The chapter starts with a single TLU, compare p. 304 in \"Hands on Machine Learning\". Go through all 7 steps which are an easy example of how Stochastic Gradient Descent works.\n",
        "\n",
        "Our goal is to train a single TLU, which can decide if one number is larger then the other one. Therefore we create 100 random pairs with pyTorch and create a target vector which is eather 1 or 0.\n"
      ],
      "metadata": {
        "id": "ETcE9B9rdcEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((100, 2))\n",
        "y = torch.where(x[:,0] > x[:,1], 1.0, 0.0)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "17qLyDnbpSbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc60397-9012-4112-9866-d740edccdf4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.9269e+00,  1.4873e+00],\n",
            "        [ 9.0072e-01, -2.1055e+00],\n",
            "        [ 6.7842e-01, -1.2345e+00],\n",
            "        [-4.3067e-02, -1.6047e+00],\n",
            "        [-7.5214e-01,  1.6487e+00],\n",
            "        [-3.9248e-01, -1.4036e+00],\n",
            "        [-7.2788e-01, -5.5943e-01],\n",
            "        [-7.6884e-01,  7.6245e-01],\n",
            "        [ 1.6423e+00, -1.5960e-01],\n",
            "        [-4.9740e-01,  4.3959e-01],\n",
            "        [-7.5813e-01,  1.0783e+00],\n",
            "        [ 8.0080e-01,  1.6806e+00],\n",
            "        [ 1.2791e+00,  1.2964e+00],\n",
            "        [ 6.1047e-01,  1.3347e+00],\n",
            "        [-2.3162e-01,  4.1759e-02],\n",
            "        [-2.5158e-01,  8.5986e-01],\n",
            "        [-1.3847e+00, -8.7124e-01],\n",
            "        [-2.2337e-01,  1.7174e+00],\n",
            "        [ 3.1888e-01, -4.2452e-01],\n",
            "        [ 3.0572e-01, -7.7459e-01],\n",
            "        [-1.5576e+00,  9.9564e-01],\n",
            "        [-8.7979e-01, -6.0114e-01],\n",
            "        [-1.2742e+00,  2.1228e+00],\n",
            "        [-1.2347e+00, -4.8791e-01],\n",
            "        [-9.1382e-01, -6.5814e-01],\n",
            "        [ 7.8024e-02,  5.2581e-01],\n",
            "        [-4.8799e-01,  1.1914e+00],\n",
            "        [-8.1401e-01, -7.3599e-01],\n",
            "        [-1.4032e+00,  3.6004e-02],\n",
            "        [-6.3477e-02,  6.7561e-01],\n",
            "        [-9.7807e-02,  1.8446e+00],\n",
            "        [-1.1845e+00,  1.3835e+00],\n",
            "        [ 1.4451e+00,  8.5641e-01],\n",
            "        [ 2.2181e+00,  5.2317e-01],\n",
            "        [ 3.4665e-01, -1.9733e-01],\n",
            "        [-1.0546e+00,  1.2780e+00],\n",
            "        [-1.7219e-01,  5.2379e-01],\n",
            "        [ 5.6622e-02,  4.2630e-01],\n",
            "        [ 5.7501e-01, -6.4172e-01],\n",
            "        [-2.2064e+00, -7.5080e-01],\n",
            "        [ 1.0868e-02, -3.3874e-01],\n",
            "        [-1.3407e+00, -5.8537e-01],\n",
            "        [ 5.3619e-01,  5.2462e-01],\n",
            "        [ 1.1412e+00,  5.1644e-02],\n",
            "        [ 7.4395e-01, -4.8158e-01],\n",
            "        [-1.0495e+00,  6.0390e-01],\n",
            "        [-1.7223e+00, -8.2777e-01],\n",
            "        [ 1.3347e+00,  4.8354e-01],\n",
            "        [-2.5095e+00,  4.8800e-01],\n",
            "        [ 7.8459e-01,  2.8647e-02],\n",
            "        [ 6.4076e-01,  5.8325e-01],\n",
            "        [ 1.0669e+00, -4.5015e-01],\n",
            "        [-1.8527e-01,  7.5276e-01],\n",
            "        [ 4.0476e-01,  1.7847e-01],\n",
            "        [ 2.6491e-01,  1.2732e+00],\n",
            "        [-1.3109e-03, -3.0360e-01],\n",
            "        [-1.4570e+00, -1.0234e-01],\n",
            "        [-5.9915e-01,  4.7706e-01],\n",
            "        [ 7.2618e-01,  9.1152e-02],\n",
            "        [-3.8907e-01,  5.2792e-01],\n",
            "        [-1.2685e-02,  2.4084e-01],\n",
            "        [ 1.3254e-01,  7.6424e-01],\n",
            "        [ 1.0950e+00,  3.3989e-01],\n",
            "        [ 7.1997e-01,  4.1141e-01],\n",
            "        [ 1.9312e+00,  1.0119e+00],\n",
            "        [-1.4364e+00, -1.1299e+00],\n",
            "        [-1.3603e-01,  1.6354e+00],\n",
            "        [ 6.5474e-01,  5.7600e-01],\n",
            "        [ 1.1415e+00,  1.8565e-02],\n",
            "        [-1.8058e+00,  9.2543e-01],\n",
            "        [-3.7534e-01,  1.0331e+00],\n",
            "        [-6.8665e-01,  6.3681e-01],\n",
            "        [-9.7267e-01,  9.5846e-01],\n",
            "        [ 1.6192e+00,  1.4506e+00],\n",
            "        [ 2.6948e-01, -2.1038e-01],\n",
            "        [-7.3280e-01,  1.0430e-01],\n",
            "        [ 3.4875e-01,  9.6759e-01],\n",
            "        [-4.6569e-01,  1.6048e+00],\n",
            "        [-2.4801e+00, -4.1754e-01],\n",
            "        [-1.1955e+00,  8.1234e-01],\n",
            "        [-1.9006e+00,  2.2858e-01],\n",
            "        [ 2.4859e-02, -3.4595e-01],\n",
            "        [ 2.8683e-01, -7.3084e-01],\n",
            "        [ 1.7482e-01, -1.0939e+00],\n",
            "        [-1.6022e+00,  1.3529e+00],\n",
            "        [ 1.2888e+00,  5.2295e-02],\n",
            "        [-1.5469e+00,  7.5671e-01],\n",
            "        [ 7.7552e-01,  2.0265e+00],\n",
            "        [ 3.5818e-02,  1.2059e-01],\n",
            "        [-8.0566e-01, -2.0758e-01],\n",
            "        [-9.3195e-01, -1.5910e+00],\n",
            "        [-1.1360e+00, -5.2260e-01],\n",
            "        [-1.5933e-01, -4.2494e-01],\n",
            "        [ 9.4423e-01, -1.8493e-01],\n",
            "        [ 1.0608e+00,  2.0830e-01],\n",
            "        [-5.7785e-01,  3.2546e-01],\n",
            "        [ 2.6178e-01, -7.5993e-01],\n",
            "        [-2.0461e+00, -1.5295e+00],\n",
            "        [ 4.0487e-01,  6.3188e-01],\n",
            "        [ 3.1253e-01, -3.3502e-02]])\n",
            "tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a function f that is a single TLU, meaning that it summarizes x with weights a, b, c:\n",
        "\n",
        "$ax_0+bx_1+c$\n",
        "\n",
        "In Addition we are using a *sigmoid()* function as step function.\n",
        "\n",
        "$f = \\text{sigmoid}(ax_0+bx_1+c)$"
      ],
      "metadata": {
        "id": "z267w4G48rxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + torch.exp(-z))\n",
        "    #calculate the sigmoid of x (z)\n",
        "\n",
        "def f(x, params):\n",
        "    a, b, c = params\n",
        "    z = a*x[:,0] + b*x[:,1] + c\n",
        "    return sigmoid(z)\n"
      ],
      "metadata": {
        "id": "a_NvBnCGoLPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to our TLU function, we need a loss function. Your task is to implement a absolute difference loss function, $∑|x_i-y_i|$, which counts the number of wrong guesses."
      ],
      "metadata": {
        "id": "UBiKkGKx-jVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mae(preds, targets):\n",
        "    return torch.mean(torch.abs(preds - targets))"
      ],
      "metadata": {
        "id": "cwzyy281wI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to train your single TLU with the absolute difference loss function, use the following code. Choose an appropriate step weight `lr` and try to explain what is happing in each line."
      ],
      "metadata": {
        "id": "eGVNErmbvFxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1 #A learning rate of 1 means that on each iteration, the parameters (weights) of the model \n",
        "#will be updated by an amount equal to the gradient of the loss function multiplied by 1. \n",
        "#so the weights are basically adjusted by the value of the gradient of the loss function \n",
        "params = torch.randn(3).requires_grad_() #initialize random weights\n",
        "#this code only works this way because it is a tensor object, which is a relatively special kind of object in python \n",
        "def apply_step(params, prn=True):\n",
        "    preds = f(x, params)\n",
        "    loss = mae(preds, y) \n",
        "    loss.backward() #calculates gradient of loss function in order to determine what change in weights allows \n",
        "    #for the fastest adjustment of weights to accurately find the pattern in the data. \n",
        "    #should the gradient of the loss function become lower, than we are probably finding better weights. \n",
        "    print(f\"params before applying learning rate: {params.grad.data}\")\n",
        "    params.data -= lr * params.grad.data #updating the weights of the tensorflow object parans with the learning rate \n",
        "    print(f\"params after applying learning rate: {params.data}\")\n",
        "    params.grad = None\n",
        "    if prn: print(params);print(loss.item())\n",
        "    return preds\n",
        "\n",
        "\n",
        "for i in range(50): apply_step(params)"
      ],
      "metadata": {
        "id": "EB5TYTNmyO3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa7763c-0413-4516-a1d0-40782f1a3799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params before applying learning rate: tensor([-0.1468,  0.0487,  0.0374])\n",
            "params after applying learning rate: tensor([-0.0680,  1.2164, -0.3552])\n",
            "tensor([-0.0680,  1.2164, -0.3552], requires_grad=True)\n",
            "0.6168573498725891\n",
            "params before applying learning rate: tensor([-0.1485,  0.0568,  0.0391])\n",
            "params after applying learning rate: tensor([ 0.0805,  1.1595, -0.3942])\n",
            "tensor([ 0.0805,  1.1595, -0.3942], requires_grad=True)\n",
            "0.5911542773246765\n",
            "params before applying learning rate: tensor([-0.1482,  0.0652,  0.0402])\n",
            "params after applying learning rate: tensor([ 0.2287,  1.0943, -0.4345])\n",
            "tensor([ 0.2287,  1.0943, -0.4345], requires_grad=True)\n",
            "0.5640749931335449\n",
            "params before applying learning rate: tensor([-0.1458,  0.0733,  0.0405])\n",
            "params after applying learning rate: tensor([ 0.3745,  1.0210, -0.4749])\n",
            "tensor([ 0.3745,  1.0210, -0.4749], requires_grad=True)\n",
            "0.5361099243164062\n",
            "params before applying learning rate: tensor([-0.1414,  0.0804,  0.0396])\n",
            "params after applying learning rate: tensor([ 0.5159,  0.9405, -0.5145])\n",
            "tensor([ 0.5159,  0.9405, -0.5145], requires_grad=True)\n",
            "0.5078785419464111\n",
            "params before applying learning rate: tensor([-0.1353,  0.0861,  0.0375])\n",
            "params after applying learning rate: tensor([ 0.6512,  0.8544, -0.5521])\n",
            "tensor([ 0.6512,  0.8544, -0.5521], requires_grad=True)\n",
            "0.480052649974823\n",
            "params before applying learning rate: tensor([-0.1281,  0.0900,  0.0343])\n",
            "params after applying learning rate: tensor([ 0.7793,  0.7644, -0.5864])\n",
            "tensor([ 0.7793,  0.7644, -0.5864], requires_grad=True)\n",
            "0.4532654583454132\n",
            "params before applying learning rate: tensor([-0.1202,  0.0921,  0.0302])\n",
            "params after applying learning rate: tensor([ 0.8995,  0.6723, -0.6166])\n",
            "tensor([ 0.8995,  0.6723, -0.6166], requires_grad=True)\n",
            "0.4280322790145874\n",
            "params before applying learning rate: tensor([-0.1122,  0.0926,  0.0254])\n",
            "params after applying learning rate: tensor([ 1.0118,  0.5797, -0.6420])\n",
            "tensor([ 1.0118,  0.5797, -0.6420], requires_grad=True)\n",
            "0.40469497442245483\n",
            "params before applying learning rate: tensor([-0.1045,  0.0918,  0.0204])\n",
            "params after applying learning rate: tensor([ 1.1163,  0.4879, -0.6624])\n",
            "tensor([ 1.1163,  0.4879, -0.6624], requires_grad=True)\n",
            "0.38340020179748535\n",
            "params before applying learning rate: tensor([-0.0973,  0.0902,  0.0154])\n",
            "params after applying learning rate: tensor([ 1.2136,  0.3977, -0.6778])\n",
            "tensor([ 1.2136,  0.3977, -0.6778], requires_grad=True)\n",
            "0.3641217350959778\n",
            "params before applying learning rate: tensor([-0.0908,  0.0881,  0.0108])\n",
            "params after applying learning rate: tensor([ 1.3044,  0.3096, -0.6886])\n",
            "tensor([ 1.3044,  0.3096, -0.6886], requires_grad=True)\n",
            "0.34671714901924133\n",
            "params before applying learning rate: tensor([-0.0849,  0.0857,  0.0066])\n",
            "params after applying learning rate: tensor([ 1.3893,  0.2239, -0.6952])\n",
            "tensor([ 1.3893,  0.2239, -0.6952], requires_grad=True)\n",
            "0.33099091053009033\n",
            "params before applying learning rate: tensor([-0.0796,  0.0832,  0.0030])\n",
            "params after applying learning rate: tensor([ 1.4689,  0.1407, -0.6982])\n",
            "tensor([ 1.4689,  0.1407, -0.6982], requires_grad=True)\n",
            "0.31674060225486755\n",
            "params before applying learning rate: tensor([-0.0748,  0.0806, -0.0002])\n",
            "params after applying learning rate: tensor([ 1.5437,  0.0602, -0.6980])\n",
            "tensor([ 1.5437,  0.0602, -0.6980], requires_grad=True)\n",
            "0.3037819564342499\n",
            "params before applying learning rate: tensor([-0.0705,  0.0780, -0.0028])\n",
            "params after applying learning rate: tensor([ 1.6143, -0.0178, -0.6952])\n",
            "tensor([ 1.6143, -0.0178, -0.6952], requires_grad=True)\n",
            "0.2919553816318512\n",
            "params before applying learning rate: tensor([-0.0667,  0.0754, -0.0050])\n",
            "params after applying learning rate: tensor([ 1.6809, -0.0933, -0.6903])\n",
            "tensor([ 1.6809, -0.0933, -0.6903], requires_grad=True)\n",
            "0.2811250686645508\n",
            "params before applying learning rate: tensor([-0.0631,  0.0730, -0.0067])\n",
            "params after applying learning rate: tensor([ 1.7441, -0.1662, -0.6835])\n",
            "tensor([ 1.7441, -0.1662, -0.6835], requires_grad=True)\n",
            "0.2711750268936157\n",
            "params before applying learning rate: tensor([-0.0599,  0.0706, -0.0082])\n",
            "params after applying learning rate: tensor([ 1.8040, -0.2368, -0.6754])\n",
            "tensor([ 1.8040, -0.2368, -0.6754], requires_grad=True)\n",
            "0.26200535893440247\n",
            "params before applying learning rate: tensor([-0.0570,  0.0683, -0.0093])\n",
            "params after applying learning rate: tensor([ 1.8609, -0.3051, -0.6660])\n",
            "tensor([ 1.8609, -0.3051, -0.6660], requires_grad=True)\n",
            "0.25352942943573\n",
            "params before applying learning rate: tensor([-0.0543,  0.0662, -0.0102])\n",
            "params after applying learning rate: tensor([ 1.9153, -0.3713, -0.6558])\n",
            "tensor([ 1.9153, -0.3713, -0.6558], requires_grad=True)\n",
            "0.24567194283008575\n",
            "params before applying learning rate: tensor([-0.0519,  0.0642, -0.0109])\n",
            "params after applying learning rate: tensor([ 1.9671, -0.4355, -0.6449])\n",
            "tensor([ 1.9671, -0.4355, -0.6449], requires_grad=True)\n",
            "0.23836751282215118\n",
            "params before applying learning rate: tensor([-0.0496,  0.0622, -0.0114])\n",
            "params after applying learning rate: tensor([ 2.0167, -0.4977, -0.6335])\n",
            "tensor([ 2.0167, -0.4977, -0.6335], requires_grad=True)\n",
            "0.23155950009822845\n",
            "params before applying learning rate: tensor([-0.0476,  0.0604, -0.0118])\n",
            "params after applying learning rate: tensor([ 2.0643, -0.5581, -0.6216])\n",
            "tensor([ 2.0643, -0.5581, -0.6216], requires_grad=True)\n",
            "0.22519882023334503\n",
            "params before applying learning rate: tensor([-0.0457,  0.0587, -0.0121])\n",
            "params after applying learning rate: tensor([ 2.1100, -0.6168, -0.6096])\n",
            "tensor([ 2.1100, -0.6168, -0.6096], requires_grad=True)\n",
            "0.21924293041229248\n",
            "params before applying learning rate: tensor([-0.0439,  0.0570, -0.0122])\n",
            "params after applying learning rate: tensor([ 2.1540, -0.6738, -0.5974])\n",
            "tensor([ 2.1540, -0.6738, -0.5974], requires_grad=True)\n",
            "0.21365493535995483\n",
            "params before applying learning rate: tensor([-0.0423,  0.0554, -0.0123])\n",
            "params after applying learning rate: tensor([ 2.1963, -0.7293, -0.5851])\n",
            "tensor([ 2.1963, -0.7293, -0.5851], requires_grad=True)\n",
            "0.20840267837047577\n",
            "params before applying learning rate: tensor([-0.0409,  0.0539, -0.0123])\n",
            "params after applying learning rate: tensor([ 2.2372, -0.7832, -0.5728])\n",
            "tensor([ 2.2372, -0.7832, -0.5728], requires_grad=True)\n",
            "0.20345783233642578\n",
            "params before applying learning rate: tensor([-0.0395,  0.0525, -0.0122])\n",
            "params after applying learning rate: tensor([ 2.2767, -0.8357, -0.5606])\n",
            "tensor([ 2.2767, -0.8357, -0.5606], requires_grad=True)\n",
            "0.19879554212093353\n",
            "params before applying learning rate: tensor([-0.0382,  0.0511, -0.0121])\n",
            "params after applying learning rate: tensor([ 2.3149, -0.8868, -0.5486])\n",
            "tensor([ 2.3149, -0.8868, -0.5486], requires_grad=True)\n",
            "0.19439372420310974\n",
            "params before applying learning rate: tensor([-0.0371,  0.0498, -0.0119])\n",
            "params after applying learning rate: tensor([ 2.3520, -0.9366, -0.5366])\n",
            "tensor([ 2.3520, -0.9366, -0.5366], requires_grad=True)\n",
            "0.19023260474205017\n",
            "params before applying learning rate: tensor([-0.0360,  0.0485, -0.0117])\n",
            "params after applying learning rate: tensor([ 2.3879, -0.9850, -0.5249])\n",
            "tensor([ 2.3879, -0.9850, -0.5249], requires_grad=True)\n",
            "0.18629437685012817\n",
            "params before applying learning rate: tensor([-0.0349,  0.0472, -0.0115])\n",
            "params after applying learning rate: tensor([ 2.4229, -1.0323, -0.5134])\n",
            "tensor([ 2.4229, -1.0323, -0.5134], requires_grad=True)\n",
            "0.18256305158138275\n",
            "params before applying learning rate: tensor([-0.0340,  0.0461, -0.0113])\n",
            "params after applying learning rate: tensor([ 2.4569, -1.0783, -0.5022])\n",
            "tensor([ 2.4569, -1.0783, -0.5022], requires_grad=True)\n",
            "0.17902399599552155\n",
            "params before applying learning rate: tensor([-0.0331,  0.0449, -0.0110])\n",
            "params after applying learning rate: tensor([ 2.4900, -1.1232, -0.4912])\n",
            "tensor([ 2.4900, -1.1232, -0.4912], requires_grad=True)\n",
            "0.17566385865211487\n",
            "params before applying learning rate: tensor([-0.0323,  0.0438, -0.0107])\n",
            "params after applying learning rate: tensor([ 2.5223, -1.1670, -0.4804])\n",
            "tensor([ 2.5223, -1.1670, -0.4804], requires_grad=True)\n",
            "0.17247049510478973\n",
            "params before applying learning rate: tensor([-0.0315,  0.0427, -0.0105])\n",
            "params after applying learning rate: tensor([ 2.5538, -1.2098, -0.4700])\n",
            "tensor([ 2.5538, -1.2098, -0.4700], requires_grad=True)\n",
            "0.16943268477916718\n",
            "params before applying learning rate: tensor([-0.0308,  0.0417, -0.0102])\n",
            "params after applying learning rate: tensor([ 2.5845, -1.2515, -0.4598])\n",
            "tensor([ 2.5845, -1.2515, -0.4598], requires_grad=True)\n",
            "0.16654013097286224\n",
            "params before applying learning rate: tensor([-0.0301,  0.0407, -0.0099])\n",
            "params after applying learning rate: tensor([ 2.6146, -1.2922, -0.4499])\n",
            "tensor([ 2.6146, -1.2922, -0.4499], requires_grad=True)\n",
            "0.16378334164619446\n",
            "params before applying learning rate: tensor([-0.0295,  0.0397, -0.0096])\n",
            "params after applying learning rate: tensor([ 2.6441, -1.3319, -0.4402])\n",
            "tensor([ 2.6441, -1.3319, -0.4402], requires_grad=True)\n",
            "0.16115352511405945\n",
            "params before applying learning rate: tensor([-0.0288,  0.0388, -0.0093])\n",
            "params after applying learning rate: tensor([ 2.6729, -1.3707, -0.4309])\n",
            "tensor([ 2.6729, -1.3707, -0.4309], requires_grad=True)\n",
            "0.15864259004592896\n",
            "params before applying learning rate: tensor([-0.0283,  0.0379, -0.0091])\n",
            "params after applying learning rate: tensor([ 2.7012, -1.4086, -0.4218])\n",
            "tensor([ 2.7012, -1.4086, -0.4218], requires_grad=True)\n",
            "0.15624304115772247\n",
            "params before applying learning rate: tensor([-0.0277,  0.0371, -0.0088])\n",
            "params after applying learning rate: tensor([ 2.7290, -1.4457, -0.4130])\n",
            "tensor([ 2.7290, -1.4457, -0.4130], requires_grad=True)\n",
            "0.15394794940948486\n",
            "params before applying learning rate: tensor([-0.0272,  0.0362, -0.0085])\n",
            "params after applying learning rate: tensor([ 2.7562, -1.4820, -0.4045])\n",
            "tensor([ 2.7562, -1.4820, -0.4045], requires_grad=True)\n",
            "0.15175089240074158\n",
            "params before applying learning rate: tensor([-0.0267,  0.0354, -0.0083])\n",
            "params after applying learning rate: tensor([ 2.7829, -1.5174, -0.3962])\n",
            "tensor([ 2.7829, -1.5174, -0.3962], requires_grad=True)\n",
            "0.1496458202600479\n",
            "params before applying learning rate: tensor([-0.0263,  0.0347, -0.0080])\n",
            "params after applying learning rate: tensor([ 2.8092, -1.5521, -0.3882])\n",
            "tensor([ 2.8092, -1.5521, -0.3882], requires_grad=True)\n",
            "0.14762727916240692\n",
            "params before applying learning rate: tensor([-0.0258,  0.0339, -0.0078])\n",
            "params after applying learning rate: tensor([ 2.8351, -1.5860, -0.3805])\n",
            "tensor([ 2.8351, -1.5860, -0.3805], requires_grad=True)\n",
            "0.14569009840488434\n",
            "params before applying learning rate: tensor([-0.0254,  0.0332, -0.0075])\n",
            "params after applying learning rate: tensor([ 2.8605, -1.6192, -0.3730])\n",
            "tensor([ 2.8605, -1.6192, -0.3730], requires_grad=True)\n",
            "0.14382950961589813\n",
            "params before applying learning rate: tensor([-0.0250,  0.0325, -0.0073])\n",
            "params after applying learning rate: tensor([ 2.8855, -1.6517, -0.3657])\n",
            "tensor([ 2.8855, -1.6517, -0.3657], requires_grad=True)\n",
            "0.14204102754592896\n",
            "params before applying learning rate: tensor([-0.0247,  0.0318, -0.0070])\n",
            "params after applying learning rate: tensor([ 2.9102, -1.6836, -0.3587])\n",
            "tensor([ 2.9102, -1.6836, -0.3587], requires_grad=True)\n",
            "0.14032059907913208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a line of code that counts the number of wrong predictions, rounding your predictions with *round()*."
      ],
      "metadata": {
        "id": "h5_LNc1o_o2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = f(x, params)\n",
        "rounded_preds = torch.round(preds)\n",
        "num_wrong = torch.sum(torch.abs(rounded_preds - y))\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if rounded_preds[i] == y[i]:\n",
        "        print(f\"Sample {i}: Correct Prediction\")\n",
        "    else:\n",
        "        print(f\"Sample {i}: Incorrect Prediction\")"
      ],
      "metadata": {
        "id": "EEUhyhyDxwMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3a672d-c332-4a03-f973-577fd9ef0c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: Correct Prediction\n",
            "Sample 1: Correct Prediction\n",
            "Sample 2: Correct Prediction\n",
            "Sample 3: Correct Prediction\n",
            "Sample 4: Correct Prediction\n",
            "Sample 5: Correct Prediction\n",
            "Sample 6: Correct Prediction\n",
            "Sample 7: Correct Prediction\n",
            "Sample 8: Correct Prediction\n",
            "Sample 9: Correct Prediction\n",
            "Sample 10: Correct Prediction\n",
            "Sample 11: Correct Prediction\n",
            "Sample 12: Incorrect Prediction\n",
            "Sample 13: Correct Prediction\n",
            "Sample 14: Correct Prediction\n",
            "Sample 15: Correct Prediction\n",
            "Sample 16: Correct Prediction\n",
            "Sample 17: Correct Prediction\n",
            "Sample 18: Correct Prediction\n",
            "Sample 19: Correct Prediction\n",
            "Sample 20: Correct Prediction\n",
            "Sample 21: Correct Prediction\n",
            "Sample 22: Correct Prediction\n",
            "Sample 23: Correct Prediction\n",
            "Sample 24: Correct Prediction\n",
            "Sample 25: Correct Prediction\n",
            "Sample 26: Correct Prediction\n",
            "Sample 27: Correct Prediction\n",
            "Sample 28: Correct Prediction\n",
            "Sample 29: Correct Prediction\n",
            "Sample 30: Correct Prediction\n",
            "Sample 31: Correct Prediction\n",
            "Sample 32: Correct Prediction\n",
            "Sample 33: Correct Prediction\n",
            "Sample 34: Correct Prediction\n",
            "Sample 35: Correct Prediction\n",
            "Sample 36: Correct Prediction\n",
            "Sample 37: Correct Prediction\n",
            "Sample 38: Correct Prediction\n",
            "Sample 39: Correct Prediction\n",
            "Sample 40: Correct Prediction\n",
            "Sample 41: Correct Prediction\n",
            "Sample 42: Correct Prediction\n",
            "Sample 43: Correct Prediction\n",
            "Sample 44: Correct Prediction\n",
            "Sample 45: Correct Prediction\n",
            "Sample 46: Correct Prediction\n",
            "Sample 47: Correct Prediction\n",
            "Sample 48: Correct Prediction\n",
            "Sample 49: Correct Prediction\n",
            "Sample 50: Correct Prediction\n",
            "Sample 51: Correct Prediction\n",
            "Sample 52: Correct Prediction\n",
            "Sample 53: Correct Prediction\n",
            "Sample 54: Correct Prediction\n",
            "Sample 55: Correct Prediction\n",
            "Sample 56: Correct Prediction\n",
            "Sample 57: Correct Prediction\n",
            "Sample 58: Correct Prediction\n",
            "Sample 59: Correct Prediction\n",
            "Sample 60: Correct Prediction\n",
            "Sample 61: Correct Prediction\n",
            "Sample 62: Correct Prediction\n",
            "Sample 63: Correct Prediction\n",
            "Sample 64: Correct Prediction\n",
            "Sample 65: Correct Prediction\n",
            "Sample 66: Correct Prediction\n",
            "Sample 67: Correct Prediction\n",
            "Sample 68: Correct Prediction\n",
            "Sample 69: Correct Prediction\n",
            "Sample 70: Correct Prediction\n",
            "Sample 71: Correct Prediction\n",
            "Sample 72: Correct Prediction\n",
            "Sample 73: Correct Prediction\n",
            "Sample 74: Correct Prediction\n",
            "Sample 75: Correct Prediction\n",
            "Sample 76: Correct Prediction\n",
            "Sample 77: Correct Prediction\n",
            "Sample 78: Correct Prediction\n",
            "Sample 79: Correct Prediction\n",
            "Sample 80: Correct Prediction\n",
            "Sample 81: Correct Prediction\n",
            "Sample 82: Correct Prediction\n",
            "Sample 83: Correct Prediction\n",
            "Sample 84: Correct Prediction\n",
            "Sample 85: Correct Prediction\n",
            "Sample 86: Correct Prediction\n",
            "Sample 87: Correct Prediction\n",
            "Sample 88: Correct Prediction\n",
            "Sample 89: Correct Prediction\n",
            "Sample 90: Correct Prediction\n",
            "Sample 91: Correct Prediction\n",
            "Sample 92: Correct Prediction\n",
            "Sample 93: Correct Prediction\n",
            "Sample 94: Correct Prediction\n",
            "Sample 95: Correct Prediction\n",
            "Sample 96: Correct Prediction\n",
            "Sample 97: Correct Prediction\n",
            "Sample 98: Correct Prediction\n",
            "Sample 99: Correct Prediction\n"
          ]
        }
      ]
    }
  ]
}